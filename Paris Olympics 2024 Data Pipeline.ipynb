{"cells":[{"cell_type":"markdown","source":["# Load Athletes"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ffc03acf-1b75-4640-a029-276b181f01d7"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Athletes_loc =  \"Files/athletes.csv\"\n","\n","Raw_Athletes = spark.read.csv(Athletes_loc, header=True, inferSchema=True, quote='\"', escape='\"')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d17c7900-9102-48fd-a27e-684ea9d28fc4"},{"cell_type":"markdown","source":["## Created Athletes Dimension\n","### 1. Added Age Column \n","### 2. Dropped some Columns\n","### 3. Changed Data Type of 'code','height' and 'weight' Column"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea6a118a-0098-4da7-868f-6436135a16f7"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType, IntegerType\n","\n","Athletes_dim  = Raw_Athletes['code','name','gender','function','country_code','country','nationality_code','nationality','height','weight','birth_date']\n","\n","Athletes_dim = Athletes_dim.withColumn('Agein2024',F.year(F.lit('2024-07-26')) - F.year('birth_date'))\n","\n","Athletes_dim = Athletes_dim.drop('birth_date','country_code','nationality_code')\n","\n","Athletes_dim = Athletes_dim.withColumn(\"code\",F.col(\"code\").cast(StringType())) \\\n","                .withColumn(\"height\",F.col(\"height\").cast(IntegerType())) \\\n","                .withColumn(\"weight\",F.col(\"weight\").cast(IntegerType()))\n","\n","display(Athletes_dim)\n","\n","Athletes_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"athletes_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"659c51ca-439f-43c1-ac3f-4d54b7688976"},{"cell_type":"markdown","source":["## Created Athletes Disciplines Dimension\n","### 1. Removing \n","####    - Brackets []\n","####    - Quotes ' \n","####    - Double Quotes \"\n","### 2. Splitting the Cleaned Column by Commas\n","### 3. Trimming Whitespace from Each Element\n","### 4. Unpivoting the Array into Individual Rows\n","### 5. Changed Data Type of 'code' and 'discipline' Column"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc76b30d-a151-403f-a3dc-0d110893b943"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","\n","Athletes_Dis_dim = Raw_Athletes['code','disciplines']\n","\n","df_cleaned = Athletes_Dis_dim.withColumn(\"cleaned_disciplines\", F.regexp_replace(\"disciplines\", r\"[\\[\\]\\'\\\"]\", \"\"))\n","\n","df_split = df_cleaned.withColumn(\"disciplines_array\", F.split(df_cleaned[\"cleaned_disciplines\"], \",\"))\n","\n","df_trimmed = df_split.withColumn(\"disciplines_array\", F.expr(\"transform(disciplines_array, x -> trim(x))\"))\n","\n","Athletes_Dis_dim = df_trimmed.withColumn(\"discipline\", F.explode(df_trimmed[\"disciplines_array\"])).select(\"code\", \"discipline\")\n","\n","Athletes_Dis_dim = Athletes_Dis_dim.withColumn(\"code\",F.col(\"code\").cast(StringType())) \\\n","                .withColumn(\"discipline\",F.col(\"discipline\").cast(StringType()))\n","\n","display(Athletes_Dis_dim)\n","\n","Athletes_Dis_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"athletes_dis_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"0175a271-84cc-476f-b151-84fa903662b1"},{"cell_type":"markdown","source":["## Created Athletes Language Dimension\n","### 1. Splitting the Language Column by Commas\n","### 2. Trimming Whitespace from Each Element\n","### 3. Unpivoting the Array into Individual Rows\n","### 4. Changed Data Type of 'code' and 'Language' Column"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"91c1559f-9b2c-40a1-92bd-d61dedfeb9b4"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","Athletes_Lang_dim = Raw_Athletes['code','lang']\n","\n","Athletes_Lang_dim_split = Athletes_Lang_dim.withColumn(\"Language_Array\",F.split(Athletes_Lang_dim[\"lang\"],\",\"))\n","\n","Athletes_Lang_dim_trim = Athletes_Lang_dim_split.withColumn(\"Langauage_Array\",F.expr(\"transform(Language_Array, x-> trim(x))\"))\n","\n","Athletes_Language_dim = Athletes_Lang_dim_trim.withColumn(\"Language\",F.explode(Athletes_Lang_dim_trim[\"Language_Array\"])).select('code','Language')\n","\n","Athletes_Language_dim = Athletes_Language_dim.withColumn(\"code\",F.col(\"code\").cast(StringType())) \\\n","                .withColumn(\"Language\",F.col(\"Language\").cast(StringType()))\n","\n","display(Athletes_Language_dim)\n","\n","Athletes_Language_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"athletes_lang_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e7b01306-5244-45fe-868b-658bb181e506"},{"cell_type":"markdown","source":["## Created Athletes Events Dimension\n","### 1. Removing\n","#### - Brackets []\n","#### - Double Quotes \"\n","### 2. Splitting the Events Column by Commas\n","### 3. Trimming Whitespace from Each Element\n","### 4. Unpivoting the Array into Individual Rows\n","### 5. Removing only the leading and trailing \n","#### - Single Quotes '\n","### 6. Changed Data Type of 'code' and 'events' Column"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"85eacf78-5d10-4e15-91d7-47fbd2ffeaa0"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","\n","Athletes_Events_dim = Raw_Athletes['code','events']\n","\n","Athletes_Events_dim_Cleaned = Athletes_Events_dim.withColumn('events_cleaned',F.regexp_replace(\"events\",r\"[\\]\\[\\\"]\",\"\"))\n","\n","Athletes_Events_dim_Split = Athletes_Events_dim_Cleaned.withColumn('events_split',F.split(Athletes_Events_dim_Cleaned['events_cleaned'],\",\"))\n","\n","Athletes_Events_dim_Trim = Athletes_Events_dim_Split.withColumn(\"events_trim\",F.expr(\"transform(events_split, x -> trim(x))\"))\n","\n","Athletes_Events_dim = Athletes_Events_dim_Trim.withColumn(\"Events\",F.explode(Athletes_Events_dim_Trim[\"events_trim\"])).select('code','Events')\n","\n","Athletes_Events_dim = Athletes_Events_dim.withColumn(\"Events\", F.regexp_replace(\"Events\", r\"(^[\\']|[\\']$)\", \"\")).select('code','Events')\n","\n","Athletes_Events_dim = Athletes_Events_dim.withColumn(\"code\",F.col(\"code\").cast(StringType())) \\\n","                        .withColumn(\"Events\",F.col(\"Events\").cast(StringType()))\n","\n","display(Athletes_Events_dim)\n","\n","Athletes_Events_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"athletes_events_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"5e9da4fd-b139-4fec-b3ac-0f67af74f7de"},{"cell_type":"markdown","source":["# Done with Athletes :)\n","# Now Load Coaches"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a171da06-4c95-4903-a381-aa359b5d57f9"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Coaches_Loc =  \"Files/coaches.csv\"\n","\n","Raw_Coaches = spark.read.csv(Coaches_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Coaches)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8f26e5e9-61d4-4415-8ca0-a25b56c0dabf"},{"cell_type":"markdown","source":["## Created Coaches Dimension\n","### 1. Added Age Column\n","### 2. Removed Unnecessary Columns\n","### 3. Changed Data type of 'code' column\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10bc25b5-98e5-4079-9ad7-1c6fcbbd2f3d"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","\n","Coaches_dim = Raw_Coaches.withColumn(\"Agein2024\", F.year(F.lit('2024-07-24')) - F.year('birth_date'))\n","\n","Coaches_dim = Coaches_dim['code','name','gender','function','country_code','country','disciplines','events','birth_date','Agein2024']\n","\n","Coaches_dim = Coaches_dim.withColumn('code',F.col('code').cast(StringType()))\n","\n","display(Coaches_dim)\n","\n","Coaches_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"coaches_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b2fa7385-5882-4a71-afd7-0d72a724c4a6"},{"cell_type":"markdown","source":["# Done with Coaches :)\n","# Now Load Events"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f8c8b1f-d2cb-4a70-a828-362eea3c61e1"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Events_Loc =  \"Files/events.csv\"\n","\n","Raw_Events = spark.read.csv(Events_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Events)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a04da6db-6a81-40c4-899d-12dffa8fa57b"},{"cell_type":"markdown","source":["## Created Events Dimension\n","### 1. Added 2 Columns:\n","#### - 'event_code' by merging 'event' column and 'sport_code' column delimited by a space (\" \")\n","#### - 'event_sport' by merging 'event' column and 'sport' column delimited by a space (\" \")\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a0923b3-fff0-4053-983e-d1961411855c"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","\n","Events_dim = Raw_Events.withColumn(\"event_code\",F.concat(F.col('event'),F.lit(\" \"),F.col('sport_code')))\n","\n","Events_dim = Events_dim.withColumn(\"event_sport\",F.concat(F.col('event'),F.lit(\" \"),F.col('sport')))\n","\n","Events_dim = Events_dim['event_code','event_sport','event','tag','sport','sport_code','sport_url']\n","\n","display(Events_dim)\n","\n","Events_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"events_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"db56e8bf-c4f2-4150-a96c-c6f357289c4a"},{"cell_type":"markdown","source":["# Done with Events :)\n","# Now Load Teams"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4fa76719-34d3-4445-b216-071b1faf3a2a"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Teams_Loc =  \"Files/teams.csv\"\n","\n","Raw_Teams = spark.read.csv(Teams_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Teams)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9164ff3e-53bd-4548-b32f-405d17710852"},{"cell_type":"markdown","source":["## Created Teams Dimension\n","### 1. Added 'event_code' column to Teams Dimension by combining 'events' column and 'disciplines_code' column delimited by a space (\" \")\n","### 2. Removed unnecessary columns. \n","### 3. Changed data type of 'num_athletes' and 'num_coaches' column\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"876ddc81-eae1-42dd-bff0-1341d1037dc3"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import IntegerType\n","\n","Teams_dim = Raw_Teams.withColumn('event_code',F.concat(F.col('events'),F.lit(\" \"),F.col('disciplines_code')))\n","\n","Teams_dim = Teams_dim['code','team','team_gender','country_code','country','event_code','events','disciplines_code','discipline','num_athletes','num_coaches']\n","\n","Teams_dim = Teams_dim.withColumn('num_athletes',F.col('num_athletes').cast(IntegerType())) \\\n","            .withColumn('num_coaches',F.col('num_coaches').cast(IntegerType()))\n","\n","display(Teams_dim)\n","\n","Teams_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"teams_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"388ff376-1996-47d5-b9a8-06e1b8bd6edb"},{"cell_type":"markdown","source":["## Created Teams Athletes Names Dimension\n","### 1. Removing\n","#### - Brackets []\n","#### - Double Quotes \"\n","#### - Quotes '\n","### 2. Splitting the Athletes Column by Commas\n","### 3. Trimming Whitespace from Each Element\n","### 4. Unpivoting the Array into Individual Rows\n","### 5. Added a new Column 'Id' with Monotonically Increasing id starting from 1"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"38b779c4-1d7a-4508-97c3-f0e4e66db74c"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","Teams_Athletes = Raw_Teams['code','athletes']\n","\n","Teams_Athletes_Cleaned = Teams_Athletes.withColumn('athletes',F.regexp_replace(\"athletes\",r\"[\\]\\[\\\"\\']\",\"\"))\n","\n","Teams_Athletes_Split = Teams_Athletes_Cleaned.withColumn('athletes',F.split(Teams_Athletes_Cleaned['athletes'],\",\"))\n","\n","Teams_Athletes_Trimmed = Teams_Athletes_Split.withColumn('athletes',F.expr(\"transform(athletes, x -> trim(x))\"))\n","\n","Teams_Athletes_dim = Teams_Athletes_Trimmed.withColumn('Athletes',F.explode(Teams_Athletes_Trimmed['athletes'])).select('code','Athletes')\n","\n","Teams_AthleteName_dim = Teams_Athletes_dim.withColumn(\"Id\", F.monotonically_increasing_id() + 1)\n","\n","display(Teams_AthleteName_dim)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b13203e1-d59b-4bdd-9634-b30e208cbd98"},{"cell_type":"markdown","source":["## Created Teams Athletes id Dimension\n","### 1. Removing\n","#### - Brackets []\n","#### - Double Quotes \"\n","#### - Quotes '\n","### 2. Splitting the Athletes Codes Column by Commas\n","### 3. Trimming Whitespace from Each Element\n","### 4. Unpivoting the Array into Individual Rows\n","### 5. Added a new Column 'Id' with Monotonically Increasing id starting from 1"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1d0f2c4f-31aa-46c0-a440-67755aa82c4c"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","Teams_Athletes_id = Raw_Teams['code','athletes_codes']\n","\n","Teams_Athletes_id_Cleaned = Teams_Athletes_id.withColumn('athletes_codes',F.regexp_replace(\"athletes_codes\",r\"[\\]\\[\\\"\\']\",\"\"))\n","\n","Teams_Athletes_id_Split = Teams_Athletes_id_Cleaned.withColumn('athletes_codes',F.split(Teams_Athletes_id_Cleaned['athletes_codes'],\",\"))\n","\n","Teams_Athletes_id_Trimmed = Teams_Athletes_id_Split.withColumn('athletes_codes',F.expr(\"transform(athletes_codes, x -> trim(x))\"))\n","\n","Teams_Athletes_dim = Teams_Athletes_id_Trimmed.withColumn('Athlete_Code',F.explode(Teams_Athletes_id_Trimmed['athletes_codes'])).select('code','Athlete_Code')\n","\n","Teams_Athlete_Id_dim = Teams_Athletes_dim.withColumn(\"Id\", F.monotonically_increasing_id() + 1)\n","\n","display(Teams_Athlete_Id_dim)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c4617701-7595-4a16-883c-169e43e4f8d8"},{"cell_type":"markdown","source":["## Created Team Athletes Dimension\n","## Before Joining change the Column name of 'code' to 'Code1' from 'Teams_Athlete_Id' Dataframe to remove ambiguity.\n","### 1. Join These two Dataframes 'Teams_Athlete_Id' and 'Teams_AthleteName_dim' using Inner Join on id column.\n","### 2. Remove Unnecessary Columns "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6410f723-23e3-429e-a422-e40185e07a8c"},{"cell_type":"code","source":["Teams_Athlete_Id_dim = Teams_Athlete_Id_dim.withColumnRenamed('code','Code1')\n","\n","Team_Athletes_dim =  Teams_Athlete_Id_dim.join(Teams_AthleteName_dim, on=\"id\", how=\"inner\")\n","\n","Team_Athletes_dim = Team_Athletes_dim['code','Athlete_Code','Athletes']\n","\n","display(Team_Athletes_dim)\n","\n","Team_Athletes_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"team_athletes_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"868acb58-6acd-42a0-844d-43368a826a71"},{"cell_type":"markdown","source":["## Created Teams Coaches Names Dimension\n","### 1. Removing\n","#### - Brackets []\n","#### - Double Quotes \"\n","#### - Quotes '\n","### 2. Splitting the Coaches Column by Commas\n","### 3. Trimming Whitespace from Each Element\n","### 4. Unpivoting the Array into Individual Rows\n","### 5. Added a new Column 'Id' with Monotonically Increasing id starting from 1"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9005902b-289f-4421-ad4b-78f8e385d9db"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","Teams_Coaches = Raw_Teams['code','coaches']\n","\n","Teams_Coaches_Cleaned = Teams_Coaches.withColumn('coaches',F.regexp_replace(\"coaches\",r\"[\\]\\[\\\"\\']\",\"\"))\n","\n","Teams_Coaches_Split = Teams_Coaches_Cleaned.withColumn('coaches',F.split(Teams_Coaches_Cleaned['coaches'],\",\"))\n","\n","Teams_Coaches_Trimmed = Teams_Coaches_Split.withColumn('coaches',F.expr(\"transform(coaches, x -> trim(x))\"))\n","\n","Teams_Coaches_dim = Teams_Coaches_Trimmed.withColumn('coach',F.explode(Teams_Coaches_Trimmed['coaches'])).select('code','coach')\n","\n","Teams_Coaches_Names_dim = Teams_Coaches_dim.withColumn(\"Id\", F.monotonically_increasing_id() + 1)\n","\n","display(Teams_Coaches_Names_dim)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e32433fb-ca46-4ebd-abc8-5e8d2bd9cb2f"},{"cell_type":"markdown","source":["## Created Teams Coaches id Dimension\n","### 1. Removing\n","#### - Brackets []\n","#### - Double Quotes \"\n","#### - Quotes '\n","### 2. Splitting the Coaches Codes Column by Commas\n","### 3. Trimming Whitespace from Each Element\n","### 4. Unpivoting the Array into Individual Rows\n","### 5. Added a new Column 'Id' with Monotonically Increasing id starting from 1"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"744228f9-05ed-4af5-b7e6-81b8614ad275"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","Teams_Coaches_id = Raw_Teams['code','coaches_codes']\n","\n","Teams_Coaches_id_Cleaned = Teams_Coaches_id.withColumn('coaches_codes',F.regexp_replace(\"coaches_codes\",r\"[\\]\\[\\\"\\']\",\"\"))\n","\n","Teams_Coaches_id_Split = Teams_Coaches_id_Cleaned.withColumn('coaches_codes',F.split(Teams_Coaches_id_Cleaned['coaches_codes'],\",\"))\n","\n","Teams_Coaches_id_Trimmed = Teams_Coaches_id_Split.withColumn('coaches_codes',F.expr(\"transform(coaches_codes, x -> trim(x))\"))\n","\n","Teams_Coaches_dim = Teams_Coaches_id_Trimmed.withColumn('coach_code',F.explode(Teams_Coaches_id_Trimmed['coaches_codes'])).select('code','coach_code')\n","\n","Teams_Coaches_Id_dim = Teams_Coaches_dim.withColumn(\"Id\", F.monotonically_increasing_id() + 1)\n","\n","display(Teams_Coaches_Id_dim)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"cd10d4f7-0776-4c0c-a4a6-4ef52fe033e6"},{"cell_type":"markdown","source":["## Created Team Coaches Dimension\n","## Before Joining change the Column name of 'code' to 'Code1' from 'Teams_Coaches_Id_dim' Dataframe to remove ambiguity.\n","### 1. Join These two Dataframes 'Teams_Coaches_Id_dim' and 'Teams_Coaches_dim' using Inner Join on id column.\n","### 2. Remove Unnecessary Columns "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"31adacd3-4dfd-4e6a-8527-2aa2393065ac"},{"cell_type":"code","source":["Teams_Coaches_Id_dim = Teams_Coaches_Id_dim.withColumnRenamed('code','Code1')\n","\n","Team_Coaches_Joined_dim =  Teams_Coaches_Id_dim.join(Teams_Coaches_Names_dim, on=\"Id\", how=\"inner\")\n","\n","Team_Coaches_dim = Team_Coaches_Joined_dim['code','coach_code','coach']\n","\n","display(Team_Coaches_dim)\n","\n","Team_Coaches_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"team_coaches_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"1db4a656-348e-4a93-a510-eae074d5dd64"},{"cell_type":"markdown","source":["# Done With Teams :)\n","# Now Load Technical Officials"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5a8ff256-8a5c-4421-b107-8526bb12fd21"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Technical_Officials_Loc =  \"Files/technical_officials.csv\"\n","\n","Raw_Technical_Officials = spark.read.csv(Technical_Officials_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Technical_Officials)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"566cffbc-76e7-4e25-b5f5-256e88367407"},{"cell_type":"markdown","source":["## Created Technical Officials Dimension\n","### 1. Removed unnecessary columns. \n","### 2. Changed data type of 'code' column\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8c9cef49-12ef-4c41-8b9d-9fc0d0cee584"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","\n","Technical_Officials_dim = Raw_Technical_Officials['code','name','gender','function','organisation_code','organisation']\n","\n","Technical_Officials_dim = Technical_Officials_dim.withColumn('code',F.col('code').cast(StringType()))\n","\n","display(Technical_Officials_dim)\n","\n","Technical_Officials_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"technical_officials_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"5a1b7fc6-30c7-45c0-9f60-f46c878f5be1"},{"cell_type":"markdown","source":["## Created Technical Officials Disciplines Dimension\n","### 1. Removing\n","#### - Brackets []\n","#### - Quotes '\n","### 2. Splitting the disciplines Column by Commas\n","### 3. Trimming Whitespace from Each Element\n","### 4. Unpivoting the Array into Individual Rows\n","### 5. Changed data type of 'code' column"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69d8d47e-d591-4fe3-98e3-be0013fb714e"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","\n","Technical_Officials_Disciplines_dim = Raw_Technical_Officials['code','disciplines']\n","\n","Technical_Officials_Disciplines_dim_Cleaned = Technical_Officials_Disciplines_dim.withColumn('disciplines',F.regexp_replace(\"disciplines\",r\"[\\]\\[\\']\",\"\"))\n","\n","Technical_Officials_Disciplines_dim_Split = Technical_Officials_Disciplines_dim_Cleaned.withColumn('disciplines',F.split('disciplines',\",\"))\n","\n","Technical_Officials_Disciplines_dim_Trimmed = Technical_Officials_Disciplines_dim_Split.withColumn('disciplines',F.expr(\"transform(disciplines, x -> trim(x))\"))\n","\n","Technical_Officials_Disciplines_dim = Technical_Officials_Disciplines_dim_Trimmed.withColumn('discipline',F.explode(Technical_Officials_Disciplines_dim_Trimmed['disciplines'])).select('code','discipline')\n","\n","Technical_Officials_Disciplines_dim = Technical_Officials_Disciplines_dim.withColumn('code',F.col('code').cast(StringType()))\n","\n","display(Technical_Officials_Disciplines_dim)\n","\n","Technical_Officials_Disciplines_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"technical_officials_dis_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"643b3079-385f-4978-be8a-490457e65648"},{"cell_type":"markdown","source":["# Done With Technical Officials :)\n","# Now Load Medals Total"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a830333d-8830-43db-bb96-67423100df61"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Medals_Total_Loc =  \"Files/medals_total.csv\"\n","\n","Raw_Medals_Total = spark.read.csv(Medals_Total_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"23b8dc89-9ef3-46de-b9e4-d951a3bdf4ec"},{"cell_type":"markdown","source":["# Create Medals Total Fact\n","### 1. Removed unnecessary columns."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"58162f44-4c34-4361-b9e0-aa94d46c3ca9"},{"cell_type":"code","source":["Medals_Total_Fact = Raw_Medals_Total['country_code','country','Gold_Medal','Silver_Medal','Bronze_Medal','Total']\n","\n","display(Medals_Total_Fact)\n","\n","Medals_Total_Fact.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"medals_total_fact\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"430fab58-2f98-4411-ab87-ca166bbc4968"},{"cell_type":"markdown","source":["# Done With Medals Total :)\n","# Now Create Medal Type Dimension from Medals\n","\n","### 1. Selected only 'medal_type' and 'medal_code' Column\n","### 2. Dropped all the duplicates along with null rows.\n","### 3. Changed data type of 'medal_code' Column"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10f96ad8-e501-4e00-b0a2-4a5cc7cd469d"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import IntegerType\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Medal_Types_Loc =  \"Files/medals.csv\"\n","\n","Raw_Medals = spark.read.csv(Medal_Types_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","Raw_Medals = Raw_Medals['medal_type','medal_code'].drop_duplicates()\n","\n","Raw_Medals = Raw_Medals.na.drop()\n","\n","Medal_Types_dim = Raw_Medals.withColumn('medal_code',F.col('medal_code').cast(IntegerType())).orderBy(F.col('medal_code'))\n","\n","display(Medal_Types_dim)\n","\n","Medal_Types_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"medals_types_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8b2a9bb8-d8f1-4242-8528-532c9856f3d4"},{"cell_type":"markdown","source":["# Done with Medal Type :)\n","# Now load Medallists"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad2c873f-3f88-42ce-912e-992a203f6156"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Medallists_Loc =  \"Files/medallists.csv\"\n","\n","Raw_Medallists = spark.read.csv(Medallists_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Medallists)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"0bb6d38e-4eb7-4412-8102-40ceef1182c1"},{"cell_type":"markdown","source":["## Created Medallists Fact\n","### 1. Removed unnecessary columns.\n","### 2. Changed data type of 'code_athlete' column\n","### 3. Added Age Column\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e36cbfc2-46a7-4204-a481-dbd5314b85bd"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType\n","\n","Medallist_Fact = Raw_Medallists['medal_type','medal_date','name','gender','country_code','country','nationality_code','nationality','team','team_gender','discipline','event','event_type','url_event','birth_date','code_athlete','code_team','is_medallist']\n","\n","Medallist_Fact = Medallist_Fact.withColumn('code_athlete',F.col('code_athlete').cast(StringType()))\n","\n","Medallist_Fact = Medallist_Fact.withColumn('Agein2024', F.year(F.lit('2024-07-26')) - F.year('birth_date'))\n","\n","display(Medallist_Fact)\n","\n","Medallist_Fact.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"medallist_fact\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"eec95235-f15c-4fd6-8121-34670c0dca9a"},{"cell_type":"markdown","source":["# Done with Medallists :)\n","# Now load Medals\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c73e12d3-974a-439a-97df-1ad0465a761a"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Medals_Loc =  \"Files/medals.csv\"\n","\n","Raw_Medals = spark.read.csv(Medals_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Medals)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"738009d6-44f4-426b-b5ca-e07234b0abfa"},{"cell_type":"markdown","source":["# Create Medal Athletes Fact\n","## 1. Remove Unnecessary Columns\n","## 2. Inner Join 'Medal_Athletes_Fact' with 'Athletes_dim' Dataframe on 'code' Column"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3b89239-d1f0-4af4-b112-999aa80eb03d"},{"cell_type":"code","source":["Medal_Athletes_Fact = Raw_Medals['medal_type','medal_date','name','gender','discipline','event','event_type','url_event','code','country_code']\n","\n","Medal_Athletes_Fact = Medal_Athletes_Fact.join(Athletes_dim,on='code',how='inner')\n","\n","display(Medal_Athletes_Fact)\n","\n","Medal_Athletes_Fact.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"medal_athletes_fact\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f3c8490e-c42d-4c4a-9b59-c903304b7a0d"},{"cell_type":"markdown","source":["# Create Medal Teams Fact\n","## 1. Remove Unnecessary Columns\n","## 2. Inner Join 'Medal_Teams_Fact' with 'Teams_dim' Dataframe on 'code' Column"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"432a7db2-b168-4152-8e30-220ee184be88"},{"cell_type":"code","source":["Medal_Teams_Fact = Raw_Medals['medal_type','medal_date','name','gender','event_type','url_event','code','country_code']\n","\n","Medal_Teams_Fact = Medal_Teams_Fact.join(Teams_dim,on ='code',how='inner')\n","\n","display(Medal_Teams_Fact)\n","\n","Medal_Teams_Fact.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"medal_teams_fact\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"377aaa02-db4f-49a5-b0a9-3e10352d1afa"},{"cell_type":"markdown","source":["# Done with Medals :)\n","# Now load Nocs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4607aed0-7f0a-4e74-8b3e-e3cbb733ee8e"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Nocs_Loc =  \"Files/nocs.csv\"\n","\n","Raw_Nocs = spark.read.csv(Nocs_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Nocs)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"7b63dcf7-30d5-49b8-8653-1fe9be2dcb75"},{"cell_type":"markdown","source":["# Create Nocs Dimension\n","\n","### 1. Remove Unnecessary Columns"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2884ceb5-c788-4d6e-a43e-be28d56854d5"},{"cell_type":"code","source":["Nocs_dim = Raw_Nocs['code','country','tag','note']\n","\n","display(Nocs_dim)\n","\n","Nocs_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"nocs_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a175a4f9-bad8-43a3-a18e-cc1a621fd9cb"},{"cell_type":"markdown","source":["# Done with Nocs :)\n","# Now load Schedules\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4bde72c9-5008-4779-9e60-245ba945746c"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Schedules_Loc =  \"Files/schedules.csv\"\n","\n","Raw_Schedules = spark.read.csv(Schedules_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Schedules)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"6d094abd-c491-4b8c-9673-6ec070e755c4"},{"cell_type":"markdown","source":["# Created Schedules Fact\n","## 1. Created 'time_diff','hours','minutes','seconds','formatted_time_diff' Columns\n","## 2. Created 'event_code' Column\n","## 3. Removed Unnecessary Columns"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f8195d2-079a-49e4-9cb5-ac2485022a95"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","Schedules_Fact = Raw_Schedules.withColumn(\"time_diff\",F.expr(\"CAST((unix_timestamp(end_date) - unix_timestamp(start_date)) AS INT)\")) \\\n","                                .withColumn(\"hours\", F.floor(F.col(\"time_diff\") / 3600)) \\\n","                                .withColumn(\"minutes\", F.floor((F.col(\"time_diff\") % 3600) / 60)) \\\n","                                .withColumn(\"seconds\", F.col(\"time_diff\") % 60) \\\n","                                .withColumn(\"formatted_time_diff\",F.concat_ws(\":\", F.col(\"hours\"), F.col(\"minutes\"), F.col(\"seconds\")))\n","                                \n","Schedules_Fact = Schedules_Fact.withColumn('event_code',F.concat(F.col('event'),F.lit(\" \"),F.col('discipline_code')))\n","\n","Schedules_Fact = Schedules_Fact['start_date','end_date','day','hours','minutes','formatted_time_diff','status','discipline_code','discipline','event_code','event','event_medal','event_type','venue','venue_code','location_code','location_description','phase','gender','url']\n","\n","display(Schedules_Fact)\n","\n","Schedules_Fact.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"schedules_fact\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"533a5691-0fa0-4327-a8f9-62bfb514bb19"},{"cell_type":"markdown","source":["# Done With Schedules Fact :)\n","# Now load Venues"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1a561fd3-cd86-4b64-8dba-5e7db89caef9"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Venues_Loc =  \"Files/venues.csv\"\n","\n","Raw_Venues = spark.read.csv(Venues_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Venues)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"6f9d92f8-ad5a-46c3-b838-adb52791bee5"},{"cell_type":"markdown","source":["# Create Venues Dimension\n","\n","## 1. Remove Unnecessary Columns"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"40575c9f-4e90-480a-adc9-1110a73b161d"},{"cell_type":"code","source":["Venues_Dimension = Raw_Venues['venue','date_start','date_end','tag','url']\n","\n","display(Venues_Dimension)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"4384b997-4076-4c35-89fd-42182b0f67d7"},{"cell_type":"markdown","source":["# Create Venues Sports Dimension\n","### 1. Removing\n","#### - Brackets []\n","#### - Quotes '\n","### 2. Splitting the sports Column by Commas\n","### 3. Trimming Whitespace from Each Element\n","### 4. Unpivoting the Array into Individual Rows"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e7c43c77-c6ff-48b9-b09c-ce2a6eb02653"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","Venues_Sport_Dimension = Raw_Venues['venue','sports']\n","\n","Venues_Sport_Dimension_Cleaned = Venues_Sport_Dimension.withColumn('sports',F.regexp_replace(\"sports\",r\"[\\]\\[\\']\",\"\"))\n","\n","Venues_Sport_Dimension_Split = Venues_Sport_Dimension_Cleaned.withColumn('sports',F.split('sports',\",\"))\n","\n","Venues_Sport_Dimension_Trimmed = Venues_Sport_Dimension_Split.withColumn('sports',F.expr(\"transform(sports, x -> trim(x))\"))\n","\n","Venues_Sport_Dimension = Venues_Sport_Dimension_Trimmed.withColumn('sport',F.explode(Venues_Sport_Dimension_Trimmed['sports'])).select('venue','sport')\n","\n","display(Venues_Sport_Dimension)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f1b27d5b-368d-463a-9254-1245b38d6cc0"},{"cell_type":"markdown","source":["# Inner Join 'Venues_Sport_Dimension' with 'Venues_Dimension' Dataframe on 'venue' Column\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8a2e6f8-bacc-4bd6-9839-a5288b53d916"},{"cell_type":"code","source":["Venues_Dim = Venues_Dimension.join(Venues_Sport_Dimension,on='venue',how = 'inner')\n","\n","display(Venues_Dim)\n","\n","Venues_Dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"venues_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"253b84ef-f91a-48ec-90fc-b0b9e6087820"},{"cell_type":"markdown","source":["# Done with Venues :)\n","# Now load Torch Route"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9046e44-cec6-4b87-84a1-8823cd84a424"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('CSV Processing').getOrCreate()\n","\n","Torch_Route_Loc =  \"Files/torch_route.csv\"\n","\n","Raw_Torch_Route = spark.read.csv(Torch_Route_Loc, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","display(Raw_Torch_Route)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"18b1302d-44f8-4f4c-a3f3-42c159780fb2"},{"cell_type":"markdown","source":["# Create Torch Route Dimension"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ecc4db86-4f85-4086-923d-c2d41332ef63"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import IntegerType\n","\n","Torch_Route_dim = Raw_Torch_Route.withColumn('stage_number',F.col('stage_number').cast(IntegerType()))\n","\n","display(Torch_Route_dim)\n","\n","Torch_Route_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"torch_route_dim\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"938ae327-39e4-46a9-8f38-414b85ed5699"},{"cell_type":"markdown","source":["# Done with Torch Route :)\n","# Now load Flags\n","## 1. Clean column names by replacing spaces and special characters with underscores"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f1844b74-07ea-4109-ad64-239f2b45dc7d"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","spark = SparkSession.builder.appName(\"CSV Processing\").getOrCreate()\n","\n","file_path = \"Files/flags_iso.csv\" \n","\n","Raw_Flags = spark.read.csv(file_path, header=True, inferSchema=True, quote='\"', escape='\"')\n","\n","Flag_dim = Raw_Flags.select([col(column).alias(column.replace(' ', '_').replace('-', '_')) for column in df.columns])\n","\n","Flag_dim.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"flag_dim\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e208558e-e13b-4053-9881-6580b1823dc2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"8940c603-abc2-46fa-9e25-574d2df7b7f2","default_lakehouse_name":"Olympics_LH","default_lakehouse_workspace_id":"f5b11d67-4d10-4dca-aaa0-dca7314d5cda"}}},"nbformat":4,"nbformat_minor":5}